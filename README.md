# NLP-resource
### 网课
* CS229: Machine Learning. <http://cs229.stanford.edu/>
* CS224N: Natural Language Processing with Deep Learning. <http://web.stanford.edu/class/cs224n/>  
* COS 584: Advanced Natural Language Processing. <https://princeton-nlp.github.io/cos484/cos584.html>
* 李宏毅-Deep Learning for Human Language Processing. <https://www.bilibili.com/video/BV1RE411g7rQ>
* NLP入门到精通 <https://www.bilibili.com/video/BV17K4y1W7yb?share_source=copy_web>

### 参考书
《自然语言处理入门》, 何晗, 人民邮电出版社

### 一些论文&博客
* Attention is All you Need. (Transformer) NIPS-2017. [paper](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)
* The Illustrated Transformer: <http://jalammar.github.io/illustrated-transformer/>  
* Transformer模型深度解读 [[微信](https://mp.weixin.qq.com/s/Ri0YnHByuXy-2E0rQRsg4w)]
* Transformer解析与代码 <http://nlp.seas.harvard.edu/2018/04/03/attention.html>
* BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. NAACL-2019. [paper](https://www.aclweb.org/anthology/N19-1423.pdf)
* The Illustrated BERT, ELMo, and co. <http://jalammar.github.io/illustrated-bert/>  
* Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. ACL-2019. [paper](https://www.aclweb.org/anthology/P19-1285.pdf)
* XLNet: Generalized Autoregressive Pretraining for Language Understanding. NIPS-2019. [paper](https://papers.nips.cc/paper/2019/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html)
* XLNet运行机制及和Bert的异同比较    [[知乎](https://zhuanlan.zhihu.com/p/70257427)]
* Unified Language Model Pre-training for Natural Language Understanding and Generation. NurIPS-2019. [paper](https://dl.acm.org/doi/pdf/10.5555/3454287.3455457)
* UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training. ICML-2020. [paper](http://proceedings.mlr.press/v119/bao20a.html)
* GialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation. ACL-2020. [paper](https://aclanthology.org/2020.acl-demos.30.pdf)
* 史上最细节的自然语言处理NLP/Transformer/BERT/Attention面试问题与答案  [[知乎](https://zhuanlan.zhihu.com/p/348373259)]

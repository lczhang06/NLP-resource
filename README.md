# NLP-resource
* CS229: Machine Learning. <http://cs229.stanford.edu/>
* CS224N: Natural Language Processing with Deep Learning. <http://web.stanford.edu/class/cs224n/>  
* 李宏毅-Deep Learning for Human Language Processing. <https://www.bilibili.com/video/BV1RE411g7rQ>
* Attention is All you Need. (Transformer) NIPS-2017. [paper](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)
* The Illustrated Transformer: <http://jalammar.github.io/illustrated-transformer/>  
* Transformer模型深度解读 [[微信](https://mp.weixin.qq.com/s/Ri0YnHByuXy-2E0rQRsg4w)]
* Transformer解析与代码 <http://nlp.seas.harvard.edu/2018/04/03/attention.html>
* BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. NAACL-2019. [paper](https://www.aclweb.org/anthology/N19-1423.pdf)
* The Illustrated BERT, ELMo, and co. <http://jalammar.github.io/illustrated-bert/>  
* Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. ACL-2019. [paper](https://www.aclweb.org/anthology/P19-1285.pdf)
* XLNet: Generalized Autoregressive Pretraining for Language Understanding. NIPS-2019. [paper](https://papers.nips.cc/paper/2019/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html)
* XLNet运行机制及和Bert的异同比较    [[知乎](https://zhuanlan.zhihu.com/p/70257427)]
* 史上最细节的自然语言处理NLP/Transformer/BERT/Attention面试问题与答案  [[知乎](https://zhuanlan.zhihu.com/p/348373259)]

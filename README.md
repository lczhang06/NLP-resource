# NLP-resource
* CS229: Machine Learning. <http://cs229.stanford.edu/>
* CS224N: Natural Language Processing with Deep Learning. <http://web.stanford.edu/class/cs224n/>  
* Attention is All you Need. (Transformer) NIPS-2017. [paper](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)
* The Illustrated Transformer: <http://jalammar.github.io/illustrated-transformer/>  
* BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. NAACL-2019. [paper](https://www.aclweb.org/anthology/N19-1423.pdf)
* The Illustrated BERT, ELMo, and co. <http://jalammar.github.io/illustrated-bert/>  
* Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. ACL-2019. [paper](https://www.aclweb.org/anthology/P19-1285.pdf)
* XLNet: Generalized Autoregressive Pretraining for Language Understanding. NIPS-2019. [paper](https://papers.nips.cc/paper/2019/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html)
* XLNet运行机制及和Bert的异同比较 [[知乎]](https://zhuanlan.zhihu.com/p/70257427)
